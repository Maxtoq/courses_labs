{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGBdJREFUeJzt3X+QXWV9x/H3h8WgwdAoiUyTMCwapIBTok21DoMEoYg/kIh2RKuymWFw7FjZ0RmRzhRX207rjCPRqa1NEeIIDIpIUEcEHEktHVESWMQYsBEWEoOyQbYJoITAt3+ck/Zm2WXPs8m599zzfF4zO3v3nuc+53v2fs/3nnPufe6jiMDMLCcH9ToAM7Nuc+Ezs+y48JlZdlz4zCw7Lnxmlh0XPjPLTtcKn6RNklZ0a32zIWlI0m0V245IunKW65n1Y60/Of8PzGMPlK4Vvog4ISLWd2t9bSTpk5JC0um9jsXSOP9nT9JcSf8iaYek/5H0w/3t8+ADEZjVT9IrgHcBD/c6FrMuW0NRq44Dfgss298Ou3mqO7b3SKU81L1W0pWSdkm6R9IrJV0s6RFJWyWd0fHYVZI2l23vl/TBSX1/XNLDkrZLOr88KlpaLjtE0mclPSTpN5K+JOlFFWP+fBnLTkkbJZ08qckLJX2tjOtOSSd2PHaRpOskjUt6QNJHZv3PK/wzcBGwez/7sR5w/s8u/yUdC7wduCAixiPimYjYOJu+OvXyzY2zgK8CLwHuAm4q41kMfBr4t462jwBvAw4DVgGXSnoNgKQzgY8CpwNLgVMmreczwCspXiWWlv1fUjHGO8rHvRS4GrhW0gs7lp8NXNuxfJ2kF0g6CPg2cHe5vtOAYUlvmmolkn4q6b3TBSHpL4DdEfHdinFb8zn/SzPk/+uAB4FPlae690h6Z8X4pxcRXfkBxoDTy9sjwC0dy84CHgcGyr/nAQHMn6avdcCF5e3LgX/sWLa0fOxSQMATwCs6lr8eeGCafoeA255nGx4DTuzYhts7lh1EcRp6cvlkPTTpsRcDV3Q89sqK/7cXA/8NHD35/+if/vlx/s86//+m3J4RYA5FYX8cOG5/no9eXuP7Tcft3wE7IuKZjr+h2OknJL0Z+CTFK9dBwFzgnrLNImBDR19bO24vLNtulLT3PgEDVQKU9DHg/HIdQfGKu2CqdUXEs5K2dbRdJGmio+0A8J9V1jvJp4CvRsQDs3isNZfzv5rfAU8Dfx8Re4D/kHQrcAaweRb9AX3w5oakQ4DrgA8AN0TE05LWUTyBULzKLOl4yJEdt3dQ/ONOiIhfJa73ZIpraqcBm8on9rGO9e6zrvLwfgmwHdhD8ap6TMo6p3EasETSX5V/LwS+LukzEfGZA9C/NZjzn58egD6eox8+wDwHOAQYB/aUr35ndCz/OrBK0nGS5tJx/SIingX+neKayMsAJC2e7lrDJPMonsBx4GBJl1C84nX6E0nnSDoYGAaeAm4HfgLslHSRpBdJGpD0Kkl/mr75nAa8iuJayzKKxPog8MVZ9GX9J/f8/yHwEHCxpIMlnQSsoLgmOmuNL3wRsQv4CMUT/BjwXuBbHctvBL4A3ApsAX5ULnqq/H1Ref/tknYC3weOrbDqm4AbgV9QXFz9PfueRgDcALy7jOv9wDkR8XR5ynIWRaF6gOKV9zLgD6ZakYoPt/7lNNv/aET8eu8P8AzwWEQ8XmEbrM85/+NpijdR3gL8D0Uh/0BE3FthG6al8gJia0g6DvgZcEh5TcAsG87/ahp/xFeFpHdImiPpJRRv33/bT7rlwvmfrhWFj+Ka1zjwS4pTwQ/1NhyzrnL+J2rdqa6Z2UzacsRnZlZZLZ/jW7BgQQwODtbR9axs3Tr5zajp7dq1K6nv448/Pqn9o48+mtT+qaeemrlRaWJigieffFIzt7Q61Z3/zzzzzMyNOtx3332V2x5++OFJfR9xxBFJ7eu2cePGHRGxcKZ2tRS+wcFBNmzYMHPDLhkeHq7cdv369Ul9p27n2rVrk9qPjY1VbrtmzZqkvq0edef/xMTEzI06rFixonLboaGhpL5T9q1ukPRglXY+1TWz7FQqfJLOlHSfpC2SPlF3UGZN4vxvnxkLn6QBiuFRbwaOB94jKe3Cllmfcv63U5UjvtcCWyLi/ojYDVxDMYTELAfO/xaqUvgWs+8YvW3lffuQdIGkDZI2jI+PH6j4zHrN+d9CVQrfVB+PeM6nniNiTUQsj4jlCxfO+G6yWb9w/rdQlcK3jX2/42vvd26Z5cD530JVCt8dwDGSjpY0BziXjq/FMWs5538LzfgB5ojYI+nDFN/PNQBcHhGbao/MrAGc/+1UaeRGFLN7eYYvy5Lzv30aP+fGVEZHR5PapwwTSx2ylip1iE/KEKLUMZzWn1KHPaYMcVu5cmViNP3JQ9bMLDsufGaWHRc+M8uOC5+ZZceFz8yy48JnZtlx4TOz7LjwmVl2XPjMLDsufGaWHRc+M8tOI8bqpk6XlzoF3rJly2qLZfXq1UntU6Vs680331xfINYYqWN1U3K0SfNh18lHfGaWHRc+M8tOleklj5R0q6TNkjZJurAbgZk1gfO/napc49sDfCwi7pQ0D9go6ZaI+HnNsZk1gfO/hWY84ouIhyPizvL2LmAzU0yvZ9ZGzv92SrrGJ2kQeDXw4ymWeV5RazXnf3tULnySXgxcBwxHxM7Jyz2vqLWZ879dKhU+SS+geNKviohv1huSWbM4/9unyru6Ar4MbI6Iz9UfkllzOP/bqcoR30nA+4E3Shotf95Sc1xmTeH8b6EqE4rfBqgLsZg1jvO/nRoxVjd17OHdd99dTyDAqaeeWlvfAOedd15S+5RxxnPnzk0Nx/pQav7nMv42hYesmVl2XPjMLDsufGaWHRc+M8uOC5+ZZceFz8yy48JnZtlx4TOz7LjwmVl2XPjMLDuNGLI2f/78pPannHJKUvuUITvr169P6jvVyMhIrf1b/xkdHa21/5TpJcfGxuoLhPTpWFOGbKbwEZ+ZZceFz8yy48JnZtlJmXNjQNJdkr5TZ0BmTeT8b5eUI74LKabWM8uR879Fqk42tAR4K3BZveGYNY/zv32qHvGtBj4OPDtdA88rai3m/G+ZKrOsvQ14JCI2Pl87zytqbeT8b6eqs6y9XdIYcA3FbFNX1hqVWXM4/1toxsIXERdHxJKIGATOBX4QEe+rPTKzBnD+t5M/x2dm2UkaqxsR64H1BzqIoaGhWtunSB0bmNreU/31r7ryv24TExOV2w4PDyf1nTq2ve7+q/IRn5llx4XPzLLjwmdm2XHhM7PsuPCZWXZc+MwsOy58ZpYdFz4zy44Ln5llx4XPzLLjwmdm2WnEvLp1W7t2beW2d999d1Lf69atS4zGbF+p47dPPPHEegIhfY7r1LG0TRmr7iM+M8uOC5+ZZafqZEPzJX1D0r2SNkt6fd2BmTWF8799ql7j+zzwvYh4l6Q5wNwaYzJrGud/y8xY+CQdBrwBGAKIiN3A7nrDMmsG5387VTnVfTkwDlxRziR/maRDJzfy9HrWUs7/FqpS+A4GXgP8a0S8GngC+MTkRp5ez1rK+d9CVQrfNmBbRPy4/PsbFIlglgPnfwtVmV7y18BWSceWd50G/LzWqMwawvnfTlXf1f1r4KryHa37gVX1hWTWOM7/lqlU+CJiFFhecyxmjeT8bx+P1Z3k7LPPTuq7KWMPrX/VPT525cqVldueeuqpSX2n7i+rV69Oal8XD1kzs+y48JlZdlz4zCw7Lnxmlh0XPjPLjgufmWXHhc/MsuPCZ2bZceEzs+y48JlZdlz4zCw7iogD36k0Djw4xaIFwI4DvsJm6sW2HhUR/hbMHnP+A73b1kr7QC2Fb9qVSRsiIotvuchpW62anHKi6dvqU10zy44Ln5llp9uFb02X19dLOW2rVZNTTjR6W7t6jc/MrAl8qmtm2XHhM7PsdKXwSTpT0n2Stkh6zmTMbSJpTNI9kkYlbeh1PNZ7OeU/9Mc+UPs1PkkDwC+AP6eYnPkO4D0R0cq5SSWNAcsjIpcPqtrzyC3/oT/2gW4c8b0W2BIR90fEbuAaIG1qJrP+5fxvoG4UvsXA1o6/t5X3tVUAN0vaKOmCXgdjPZdb/kMf7APdmFdXU9zX5s/QnBQR2yW9DLhF0r0R8cNeB2U9k1v+Qx/sA9044tsGHNnx9xJgexfW2xMRsb38/QhwPcWpjuUrq/yH/tgHulH47gCOkXS0pDnAucC3urDerpN0qKR5e28DZwA/621U1mPZ5D/0zz5Q+6luROyR9GHgJmAAuDwiNtW93h45ArheEhT/26sj4nu9Dcl6KbP8hz7ZBzxkzcyy45EbZpYdFz4zy44Ln5llx4XPzLLjwmdm2XHhM7PsuPCZWXZc+MwsOy58ZpYdFz4zy44Ln5llx4XPzLLTtcInaZOkFd1a32xIGpJ0W8W2I5KunOV6Zv1Y60/O/wPz2AOla4UvIk6IiPXdWl9bSPozSbdI+q2kcUnXSvrDXsdlaZz/syfpNEn3SnpS0q2SjtrfPn2q23wvAdYAg8BRwC7gil4GZNYtkhYA3wT+FngpsAH42v72281T3TFJp5e3R8ojlysl7Srn4HylpIslPSJpq6QzOh67StLmsu39kj44qe+PS3pY0nZJ50sKSUvLZYdI+qykhyT9RtKXJL2oYsyfL2PZWU6ccvKkJi+U9LUyrjslndjx2EWSriuP0h6Q9JHZ/N8i4saIuDYidkbEk8A/AyfNpi/rHef/7PIfOAfYVO4DvwdGgBMl/dEs+wN6e8R3FvBViiOauyi+ofYgihmoPg38W0fbR4C3AYcBq4BLJb0GismagY8CpwNLgVMmreczwCuBZeXyxcAlFWO8o3zcS4GrgWslvbBj+dnAtR3L10l6gaSDgG8Dd5frOw0YlvSmqVYi6aeS3lsxpjcAbf4G31w4/0sz5P8JZT8ARMQTwC/L+2cvIrryA4wBp5e3R4BbOpadBTwODJR/z6OYiWr+NH2tAy4sb18O/GPHsqXlY5dSzHD1BPCKjuWvBx6Ypt8h4Lbn2YbHgBM7tuH2jmUHAQ8DJwOvAx6a9NiLgSs6HnvlLP6Hfwz8Fji5W8+bfw7Mj/N/dvkPfBn4p0n3/RcwtD/PRzeml5zObzpu/w7YERHPdPwN8GJgQtKbgU9SvHIdBMwF7inbLKI479+rcw7ThWXbjdL/zfInirkPZiTpY8D55TqC4hV3wVTriohnJW3raLtI0kRH2wHgP6usd5pYlgI3UiT8rPuxxnD+V/N4ud5Oh1Fc6561Xha+SiQdAlwHfAC4ISKelrSO/5+v9GGKKfv26pzKbwdFEp0QEb9KXO/JwEUUh+mbyif2sY717rOu8vB+79SBeyheVY9JWefzxHIU8H3g7yLiqweiT+sPzn82Aed1rOdQ4BXs5+WefnhXdw5wCDAO7Clf/c7oWP51YJWk4yTNpeP6RUQ8C/w7xTWRlwFIWjzdtYZJ5lE8gePAwZIu4bmvPH8i6RxJBwPDwFPA7cBPgJ2SLpL0IkkDkl4l6U9TN17SYuAHwBcj4kupj7e+l3X+U8zL+ypJ7yyvL14C/DQi7p1FX/+n8YUvInYBH6F4gh8D3kvHvKQRcSPwBeBWYAvwo3LRU+Xvi8r7b5e0k+LI6dgKq76J4tTyF8CDwO/Z9zQC4Abg3WVc7wfOiYiny1OWsyguDD9A8cp7GfAHU61IxYdb/3KaOM4HXg58UtLje38qxG8tkHv+R8Q48E7gH8r1vI5ibuL90rrpJSUdRzGB8SERsafX8Zh1k/O/msYf8VUh6R2S5kh6CcXb99/2k265cP6na0XhAz5IcS3il8AzwId6G45ZVzn/E7XuVNfMbCZtOeIzM6usls/xLViwIAYHB+voGoBHH300qf327dsrtz388MOT+l60aFFS+zqNjY2xY8cOzdzS6lR3/o+NjSW137On+uW+efPmJfV9xBFHJLWv28aNG3dExMKZ2tVS+AYHB9mwYcPMDWdp7dq1Se1HRkYqtx0aGqqt77otX7681yEY9ed/ao5OTEzM3Ki0YsWKpL6Hh4eT2tdN0oNV2vlU18yyU6nwSTpT0n2Stkj6RN1BmTWJ8799Zix8kgaALwJvBo4H3iPp+LoDM2sC5387VTniey2wJSLuj4jdwDUU38NllgPnfwtVKXyL2XeM3rbyvn1IukDSBkkbxsfHD1R8Zr3m/G+hKoVvqo9HPOdTzxGxJiKWR8TyhQtnfDfZrF84/1uoSuHbxr7f8bX3O7fMcuD8b6Eqhe8O4BhJR0uaQ/GVMN+a4TFmbeH8b6EZP8AcEXskfZji+7kGgMsjwpPdWBac/+1UaeRGRHwX+G7NsZg1kvO/fRox50bqELRVq1YltT/vvPNmbjTLWFKH+KS2t/Zbv359UvuvfOUr9QQC3HDDDUntR0dHk9qn7l918ZA1M8uOC5+ZZceFz8yy48JnZtlx4TOz7LjwmVl2XPjMLDsufGaWHRc+M8uOC5+ZZceFz8yy04ixuuvWrUtqf/311ye1X7lyZS1tzfrBFVdcUbnt/Pnzk/pOneoydU7guuYn9hGfmWWnyixrR0q6VdJmSZskXdiNwMyawPnfTlVOdfcAH4uIOyXNAzZKuiUifl5zbGZN4PxvoRmP+CLi4Yi4s7y9C9jMFLNMmbWR87+dkq7xSRoEXg38uI5gzJrM+d8elQufpBcD1wHDEbFziuWeV9Ray/nfLpUKn6QXUDzpV0XEN6dq43lFra2c/+1T5V1dAV8GNkfE5+oPyaw5nP/tVOWI7yTg/cAbJY2WP2+pOS6zpnD+t1CVeXVvA9SFWMwax/nfTh65YWbZacRY3eHh4aT2qeP9Utqnziu6evXqpPZmk6XOq3vKKacktU8dT5sidZ7c1PYjIyNJ7avyEZ+ZZceFz8yy48JnZtlx4TOz7LjwmVl2XPjMLDsufGaWHRc+M8uOC5+ZZceFz8yy04ghaytWrKi1/2XLllVue+mllyb1Xdf0d2bTcc7tPx/xmVl2XPjMLDspc24MSLpL0nfqDMisiZz/7ZJyxHchxdR6Zjly/rdI1cmGlgBvBS6rNxyz5nH+t0/VI77VwMeBZ6dr4On1rMWc/y1TZZa1twGPRMTG52vn6fWsjZz/7VR1lrW3SxoDrqGYberKWqMyaw7nfwvNWPgi4uKIWBIRg8C5wA8i4n21R2bWAM7/dvLn+MwsO0lD1iJiPbC+lkjMGs753x6NGKubKnVKx4mJicptU6e6NNtfqWNv161bl9Q+Jf9Tp7pM3V+aMh2rT3XNLDsufGaWHRc+M8uOC5+ZZceFz8yy48JnZtlx4TOz7LjwmVl2XPjMLDsufGaWHRc+M8tOX47VTR2rmDJv78jISFowiVLnEK57zmHrvaGhoaT2qTmakkNjY2O19T2b9nXxEZ+ZZceFz8yyU3WWtfmSviHpXkmbJb2+7sDMmsL53z5Vr/F9HvheRLxL0hxgbo0xmTWN879lZix8kg4D3gAMAUTEbmB3vWGZNYPzv52qnOq+HBgHrpB0l6TLJB06uZHnFbWWcv63UJXCdzDwGuBfI+LVwBPAJyY38ryi1lLO/xaqUvi2Adsi4sfl39+gSASzHDj/W6jKvLq/BrZKOra86zTg57VGZdYQzv92qvqu7l8DV5XvaN0PrKovJLPGcf63TKXCFxGjwPKaYzFrJOd/+/TlWN2VK1cmtW/KXJ6QPs54dHS0pkisX6XOZZuSc6l91z22vS4esmZm2XHhM7PsuPCZWXZc+MwsOy58ZpYdFz4zy44Ln5llx4XPzLLjwmdm2XHhM7PsuPCZWXYUEQe+U2kceHCKRQuAHQd8hc3Ui209KiL8LZg95vwHeretlfaBWgrftCuTNkREFt9ykdO2WjU55UTTt9WnumaWHRc+M8tOtwvfmi6vr5dy2larJqecaPS2dvUan5lZE/hU18yy48JnZtnpSuGTdKak+yRtkfScyZjbRNKYpHskjUra0Ot4rPdyyn/oj32g9mt8kgaAXwB/TjE58x3AeyKilXOTShoDlkdELh9UteeRW/5Df+wD3Tjiey2wJSLuj4jdwDXA2V1Yr1kTOP8bqBuFbzGwtePvbeV9bRXAzZI2Srqg18FYz+WW/9AH+0A35tXVFPe1+TM0J0XEdkkvA26RdG9E/LDXQVnP5Jb/0Af7QDeO+LYBR3b8vQTY3oX19kREbC9/PwJcT3GqY/nKKv+hP/aBbhS+O4BjJB0taQ5wLvCtLqy36yQdKmne3tvAGcDPehuV9Vg2+Q/9sw/UfqobEXskfRi4CRgALo+ITXWvt0eOAK6XBMX/9uqI+F5vQ7Jeyiz/oU/2AQ9ZM7PseOSGmWXHhc/MsuPCZ2bZceEzs+y48JlZdlz4zCw7Lnxmlp3/BYLQVX20UqnVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matrices\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        # the hidden activation function used\n",
    "        self.activation = activation\n",
    "        # arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        # train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        # dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later\n",
    "        self.step = None\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    # A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.zeros(X.shape)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.zeros(X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return np.zeros(X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return np.zeros(X.shape)\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.zeros(X.shape)\n",
    "    \n",
    "    # Loss function\n",
    "    # Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        return 0.\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    # Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        z_h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        # TODO:\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                # TODO:\n",
    "                pass\n",
    "            else:\n",
    "                # TODO:\n",
    "                pass\n",
    "        # TODO:\n",
    "        y = np.zeros((X.shape[0], self.output_size)) if len(X.shape) > 1 else np.zeros(self.output_size)        \n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    # Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                \n",
    "                # TODO:\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            pass\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "#### - Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
